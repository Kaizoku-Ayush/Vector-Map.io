{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "357230d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d31266",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./preprocessed_patches\") \n",
    "batch_size = 2\n",
    "learning_rate = 0.0001\n",
    "max_epochs = 25 \n",
    "rgb_channels = 3 \n",
    "segmentation_classes = 1 \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0640c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatellitePatchDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = Path(path)\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        self.transform=transform\n",
    "        for city_dir in self.path.iterdir():\n",
    "            if city_dir.is_dir():\n",
    "                for img_path in city_dir.glob(\"patch_*.png\"):\n",
    "                    if \"_gt.png\" not in img_path.name:\n",
    "                        mask_path = img_path.parent / f\"{img_path.stem}_gt.png\"\n",
    "                        if mask_path.exists():\n",
    "                            self.image_paths.append(img_path)\n",
    "                            self.mask_paths.append(mask_path)\n",
    "                        else:\n",
    "                            print(f\"Warning: Corresponding mask not found for {img_path}\")\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} image-mask pairs across all cities.\")\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        image = np.array(image, dtype=np.float32) / 255.0 \n",
    "        mask = np.array(mask, dtype=np.float32) / 255.0 \n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        image_tensor = torch.from_numpy(image)\n",
    "        mask_tensor = torch.from_numpy(mask)    \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor) \n",
    "            mask_tensor = self.transform(mask_tensor)\n",
    "\n",
    "        return image_tensor, mask_tensor    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc074e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. U-Net Model Architecture ---\n",
    "\n",
    "# Double Convolution Block\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "# Downsampling Block\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "# Upsampling Block\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1) \n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Full U-Net Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378281ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SatellitePatchDataset(path)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=rgb_channels, n_classes=segmentation_classes)\n",
    "model.to(device) # Move model to GPU \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # Binary classification \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for a progress bar in Jupyter\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{max_epochs}\")):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0) # Accumulate batch loss\n",
    "        \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
